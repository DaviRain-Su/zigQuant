# ZigQuant æ€§èƒ½ä¼˜åŒ–æŒ‡å—

> ä½å»¶è¿Ÿã€é«˜ååé‡çš„ç³»ç»Ÿä¼˜åŒ–ç­–ç•¥

---

## ğŸ¯ æ€§èƒ½ç›®æ ‡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Performance Targets                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Metric                Target          Current           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚  Order Latency (P99)    < 10ms          ?                â”‚
â”‚  WebSocket Throughput   > 100K msg/s    ?                â”‚
â”‚  Backtest Speed         > 100K ticks/s  ?                â”‚
â”‚  Memory Usage           < 500MB         ?                â”‚
â”‚  CPU Usage (1 core)     < 50%           ?                â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. ç¼–è¯‘ä¼˜åŒ–

### 1.1 ç¼–è¯‘é€‰é¡¹

```zig
// build.zig

pub fn build(b: *std.Build) void {
    const optimize = b.standardOptimizeOption(.{});

    const exe = b.addExecutable(.{
        .name = "zigquant",
        .root_source_file = .{ .path = "src/main.zig" },
        .optimize = optimize,
    });

    // ReleaseFast æ¨¡å¼é…ç½®
    if (optimize == .ReleaseFast) {
        // å¯ç”¨ LTO (Link Time Optimization)
        exe.want_lto = true;

        // å¯ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–
        exe.strip = true;  // ç§»é™¤è°ƒè¯•ç¬¦å·

        // ä½¿ç”¨ç‰¹å®š CPU ç‰¹æ€§
        exe.code_model = .small;
        exe.link_libc = false;  // å‡å°‘ä¾èµ–
    }

    // ReleaseSafe æ¨¡å¼ (æ¨èç”Ÿäº§ç¯å¢ƒ)
    if (optimize == .ReleaseSafe) {
        exe.want_lto = true;

        // ä¿ç•™å…³é”®å®‰å…¨æ£€æŸ¥
        // - æ•°ç»„è¶Šç•Œæ£€æŸ¥
        // - æ•´æ•°æº¢å‡ºæ£€æŸ¥
        // - æœªå®šä¹‰è¡Œä¸ºæ£€æŸ¥
    }
}
```

### 1.2 CPU ç‰¹æ€§ä¼˜åŒ–

```bash
# é’ˆå¯¹ç‰¹å®š CPU æ¶æ„ç¼–è¯‘
zig build -Dtarget=native -Doptimize=ReleaseFast

# ä½¿ç”¨ AVX2 æŒ‡ä»¤é›†
zig build -Dcpu=x86_64-v3 -Doptimize=ReleaseFast

# æŸ¥çœ‹ç”Ÿæˆçš„æ±‡ç¼–ä»£ç 
zig build-exe src/main.zig -femit-asm=output.s -O ReleaseFast
```

---

## 2. å†…å­˜ä¼˜åŒ–

### 2.1 Arena Allocator ä½¿ç”¨

```zig
// src/core/memory.zig

pub const MemoryPool = struct {
    arena: std.heap.ArenaAllocator,
    allocator: std.mem.Allocator,

    pub fn init(backing_allocator: std.mem.Allocator) MemoryPool {
        return .{
            .arena = std.heap.ArenaAllocator.init(backing_allocator),
            .allocator = undefined,
        };
    }

    pub fn deinit(self: *MemoryPool) void {
        self.arena.deinit();
    }

    pub fn getAllocator(self: *MemoryPool) std.mem.Allocator {
        return self.arena.allocator();
    }

    /// é‡ç½® arena (é‡Šæ”¾æ‰€æœ‰åˆ†é…)
    pub fn reset(self: *MemoryPool) void {
        _ = self.arena.reset(.retain_capacity);
    }
};

// ä½¿ç”¨ç¤ºä¾‹ï¼šæ¯ä¸ªäº¤æ˜“å‘¨æœŸä½¿ç”¨ç‹¬ç«‹çš„ arena
pub const TradingCycle = struct {
    pool: MemoryPool,

    pub fn init(allocator: std.mem.Allocator) TradingCycle {
        return .{
            .pool = MemoryPool.init(allocator),
        };
    }

    pub fn tick(self: *TradingCycle) void {
        // ä½¿ç”¨ arena allocator å¤„ç†æœ¬å‘¨æœŸçš„ä¸´æ—¶æ•°æ®
        const cycle_allocator = self.pool.getAllocator();

        // å¤„ç†è®¢å•ç°¿æ›´æ–°
        const orderbook = Orderbook.init(cycle_allocator, pair);
        // ... å¤„ç†é€»è¾‘

        // å‘¨æœŸç»“æŸæ—¶é‡ç½®ï¼Œé‡Šæ”¾æ‰€æœ‰ä¸´æ—¶åˆ†é…
        self.pool.reset();
    }
};
```

### 2.2 å¯¹è±¡æ± æ¨¡å¼

```zig
// src/core/object_pool.zig

pub fn ObjectPool(comptime T: type) type {
    return struct {
        const Self = @This();

        allocator: std.mem.Allocator,
        pool: std.ArrayList(*T),
        in_use: std.AutoHashMap(*T, void),
        max_size: usize,

        pub fn init(allocator: std.mem.Allocator, initial_size: usize, max_size: usize) !Self {
            var pool = std.ArrayList(*T).init(allocator);
            try pool.ensureTotalCapacity(initial_size);

            // é¢„åˆ†é…å¯¹è±¡
            var i: usize = 0;
            while (i < initial_size) : (i += 1) {
                const obj = try allocator.create(T);
                try pool.append(obj);
            }

            return .{
                .allocator = allocator,
                .pool = pool,
                .in_use = std.AutoHashMap(*T, void).init(allocator),
                .max_size = max_size,
            };
        }

        pub fn deinit(self: *Self) void {
            for (self.pool.items) |obj| {
                self.allocator.destroy(obj);
            }
            self.pool.deinit();
            self.in_use.deinit();
        }

        pub fn acquire(self: *Self) !*T {
            if (self.pool.items.len > 0) {
                const obj = self.pool.pop();
                try self.in_use.put(obj, {});
                return obj;
            }

            // æ± å·²ç©ºï¼Œåˆ›å»ºæ–°å¯¹è±¡
            if (self.in_use.count() < self.max_size) {
                const obj = try self.allocator.create(T);
                try self.in_use.put(obj, {});
                return obj;
            }

            return error.PoolExhausted;
        }

        pub fn release(self: *Self, obj: *T) void {
            _ = self.in_use.remove(obj);

            // é‡ç½®å¯¹è±¡çŠ¶æ€
            if (@hasDecl(T, "reset")) {
                obj.reset();
            }

            self.pool.append(obj) catch {
                // æ± å·²æ»¡ï¼Œé”€æ¯å¯¹è±¡
                self.allocator.destroy(obj);
            };
        }
    };
}

// ä½¿ç”¨ç¤ºä¾‹
const OrderPool = ObjectPool(Order);

var order_pool = try OrderPool.init(allocator, 100, 1000);
defer order_pool.deinit();

// è·å–å¯¹è±¡
const order = try order_pool.acquire();
// ä½¿ç”¨ order...

// å½’è¿˜å¯¹è±¡
order_pool.release(order);
```

### 2.3 å†…å­˜ä½¿ç”¨ç›‘æ§

```zig
// src/performance/memory_profiler.zig

pub const MemoryProfiler = struct {
    allocator: std.mem.Allocator,
    samples: std.ArrayList(MemorySample),
    start_rss: usize,

    pub const MemorySample = struct {
        timestamp: i64,
        rss: usize,  // Resident Set Size
        heap: usize,
        stack: usize,
    };

    pub fn init(allocator: std.mem.Allocator) !MemoryProfiler {
        return .{
            .allocator = allocator,
            .samples = std.ArrayList(MemorySample).init(allocator),
            .start_rss = try getCurrentRSS(),
        };
    }

    pub fn sample(self: *MemoryProfiler) !void {
        try self.samples.append(.{
            .timestamp = std.time.milliTimestamp(),
            .rss = try getCurrentRSS(),
            .heap = getHeapUsage(),
            .stack = getStackUsage(),
        });
    }

    fn getCurrentRSS() !usize {
        if (std.builtin.os.tag == .linux) {
            const file = try std.fs.openFileAbsolute("/proc/self/statm", .{});
            defer file.close();

            var buf: [256]u8 = undefined;
            const bytes_read = try file.readAll(&buf);
            const content = buf[0..bytes_read];

            var iter = std.mem.splitScalar(u8, content, ' ');
            _ = iter.next(); // total
            const rss_pages = iter.next() orelse return 0;

            const pages = try std.fmt.parseInt(usize, rss_pages, 10);
            return pages * std.mem.page_size;
        }

        return 0;
    }

    pub fn report(self: *MemoryProfiler) void {
        if (self.samples.items.len == 0) return;

        const latest = self.samples.getLast();
        const peak = blk: {
            var max = self.samples.items[0];
            for (self.samples.items) |sample| {
                if (sample.rss > max.rss) max = sample;
            }
            break :blk max;
        };

        std.debug.print("Memory Usage:\n", .{});
        std.debug.print("  Current RSS: {d} MB\n", .{latest.rss / 1024 / 1024});
        std.debug.print("  Peak RSS: {d} MB\n", .{peak.rss / 1024 / 1024});
        std.debug.print("  Delta: +{d} MB\n", .{(latest.rss - self.start_rss) / 1024 / 1024});
    }
};
```

---

## 3. æ•°æ®ç»“æ„ä¼˜åŒ–

### 3.1 é«˜æ€§èƒ½è®¢å•ç°¿

```zig
// src/market/fast_orderbook.zig

pub const FastOrderbook = struct {
    // ä½¿ç”¨å›ºå®šå¤§å°æ•°ç»„ + çº¢é»‘æ ‘æ··åˆç»“æ„
    bids_top: [100]PriceLevel,  // å‰100æ¡£ä½¿ç”¨æ•°ç»„
    asks_top: [100]PriceLevel,
    bids_deep: RBTree(PriceLevel),  // æ·±åº¦æ¡£ä½ä½¿ç”¨æ ‘
    asks_deep: RBTree(PriceLevel),

    bids_count: usize,
    asks_count: usize,

    pub fn update(self: *FastOrderbook, level: PriceLevel, side: Side) !void {
        const top_array = if (side == .bid) &self.bids_top else &self.asks_top;
        const count = if (side == .bid) &self.bids_count else &self.asks_count;

        if (count.* < 100) {
            // ç›´æ¥æ’å…¥æ•°ç»„
            top_array[count.*] = level;
            count.* += 1;

            // ä¿æŒæ’åº
            self.sortTopLevels(side);
        } else {
            // æ’å…¥æ ‘
            const tree = if (side == .bid) &self.bids_deep else &self.asks_deep;
            try tree.insert(level);
        }
    }

    pub fn getBestBid(self: *FastOrderbook) ?PriceLevel {
        if (self.bids_count > 0) {
            return self.bids_top[0];
        }
        return null;
    }

    fn sortTopLevels(self: *FastOrderbook, side: Side) void {
        const array = if (side == .bid) &self.bids_top else &self.asks_top;
        const count = if (side == .bid) self.bids_count else self.asks_count;

        // ä½¿ç”¨æ’å…¥æ’åºï¼Œé€‚åˆå°æ•°ç»„
        var i: usize = 1;
        while (i < count) : (i += 1) {
            const key = array[i];
            var j: usize = i;
            while (j > 0 and shouldSwap(array[j - 1], key, side)) : (j -= 1) {
                array[j] = array[j - 1];
            }
            array[j] = key;
        }
    }
};
```

### 3.2 ç¼“å­˜å‹å¥½çš„æ•°æ®å¸ƒå±€

```zig
// src/core/data_layout.zig

// ä¸å¥½çš„è®¾è®¡ï¼šæŒ‡é’ˆè¿½é€
pub const OrderBad = struct {
    id: []const u8,
    pair: *TradingPair,  // æŒ‡é’ˆ
    price: *Decimal,     // æŒ‡é’ˆ
    amount: *Decimal,    // æŒ‡é’ˆ
    // ... æ›´å¤šå­—æ®µ
};

// å¥½çš„è®¾è®¡ï¼šæ•°æ®å±€éƒ¨æ€§
pub const OrderGood = struct {
    // å¸¸ç”¨å­—æ®µæ”¾åœ¨å‰é¢
    price: Decimal,      // å†…è”
    amount: Decimal,     // å†…è”
    side: Side,
    status: OrderStatus,

    // ä¸å¸¸ç”¨å­—æ®µæ”¾åé¢
    id: [32]u8,          // å›ºå®šå¤§å°
    pair: TradingPair,   // å†…è”
    created_at: i64,
    // ...
};

// ç»“æ„ä½“æ•°ç»„ä¼˜äºæŒ‡é’ˆæ•°ç»„
pub const OrderBook = struct {
    // ä¸å¥½ï¼šæŒ‡é’ˆæ•°ç»„
    orders_bad: std.ArrayList(*Order),

    // å¥½ï¼šå€¼æ•°ç»„
    orders_good: std.ArrayList(Order),
};
```

---

## 4. å¹¶å‘ä¼˜åŒ–

### 4.1 æ— é”æ•°æ®ç»“æ„

```zig
// src/core/lock_free_queue.zig

pub fn LockFreeQueue(comptime T: type) type {
    return struct {
        const Self = @This();
        const Node = struct {
            data: T,
            next: std.atomic.Value(?*Node),
        };

        head: std.atomic.Value(*Node),
        tail: std.atomic.Value(*Node),
        allocator: std.mem.Allocator,

        pub fn init(allocator: std.mem.Allocator) !Self {
            const dummy = try allocator.create(Node);
            dummy.* = .{
                .data = undefined,
                .next = std.atomic.Value(?*Node).init(null),
            };

            return .{
                .head = std.atomic.Value(*Node).init(dummy),
                .tail = std.atomic.Value(*Node).init(dummy),
                .allocator = allocator,
            };
        }

        pub fn enqueue(self: *Self, data: T) !void {
            const node = try self.allocator.create(Node);
            node.* = .{
                .data = data,
                .next = std.atomic.Value(?*Node).init(null),
            };

            while (true) {
                const tail = self.tail.load(.acquire);
                const next = tail.next.load(.acquire);

                if (tail == self.tail.load(.acquire)) {
                    if (next == null) {
                        if (tail.next.cmpxchgWeak(
                            null,
                            node,
                            .release,
                            .acquire,
                        ) == null) {
                            _ = self.tail.cmpxchgWeak(tail, node, .release, .acquire);
                            return;
                        }
                    } else {
                        _ = self.tail.cmpxchgWeak(tail, next.?, .release, .acquire);
                    }
                }
            }
        }

        pub fn dequeue(self: *Self) ?T {
            while (true) {
                const head = self.head.load(.acquire);
                const tail = self.tail.load(.acquire);
                const next = head.next.load(.acquire);

                if (head == self.head.load(.acquire)) {
                    if (head == tail) {
                        if (next == null) {
                            return null;  // é˜Ÿåˆ—ä¸ºç©º
                        }
                        _ = self.tail.cmpxchgWeak(tail, next.?, .release, .acquire);
                    } else {
                        const data = next.?.data;
                        if (self.head.cmpxchgWeak(head, next.?, .release, .acquire) == head) {
                            self.allocator.destroy(head);
                            return data;
                        }
                    }
                }
            }
        }
    };
}
```

### 4.2 çº¿ç¨‹æ± 

```zig
// src/core/thread_pool.zig

pub const ThreadPool = struct {
    allocator: std.mem.Allocator,
    threads: []std.Thread,
    queue: LockFreeQueue(Task),
    running: std.atomic.Value(bool),

    pub const Task = struct {
        func: *const fn (*anyopaque) void,
        data: *anyopaque,
    };

    pub fn init(allocator: std.mem.Allocator, num_threads: u32) !ThreadPool {
        var threads = try allocator.alloc(std.Thread, num_threads);
        const queue = try LockFreeQueue(Task).init(allocator);

        var pool = ThreadPool{
            .allocator = allocator,
            .threads = threads,
            .queue = queue,
            .running = std.atomic.Value(bool).init(true),
        };

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for (threads, 0..) |*thread, i| {
            thread.* = try std.Thread.spawn(.{}, worker, .{&pool});
        }

        return pool;
    }

    fn worker(pool: *ThreadPool) void {
        while (pool.running.load(.acquire)) {
            if (pool.queue.dequeue()) |task| {
                task.func(task.data);
            } else {
                std.time.sleep(1 * std.time.ns_per_ms);
            }
        }
    }

    pub fn submit(self: *ThreadPool, func: *const fn (*anyopaque) void, data: *anyopaque) !void {
        try self.queue.enqueue(.{ .func = func, .data = data });
    }

    pub fn shutdown(self: *ThreadPool) void {
        self.running.store(false, .release);

        for (self.threads) |thread| {
            thread.join();
        }
    }
};
```

---

## 5. I/O ä¼˜åŒ–

### 5.1 æ‰¹é‡å¤„ç†

```zig
// src/network/batch_processor.zig

pub const BatchProcessor = struct {
    buffer: std.ArrayList(Message),
    flush_size: usize,
    flush_interval: i64,
    last_flush: i64,

    pub fn init(allocator: std.mem.Allocator, flush_size: usize, flush_interval_ms: i64) BatchProcessor {
        return .{
            .buffer = std.ArrayList(Message).init(allocator),
            .flush_size = flush_size,
            .flush_interval = flush_interval_ms * std.time.ns_per_ms,
            .last_flush = std.time.nanoTimestamp(),
        };
    }

    pub fn add(self: *BatchProcessor, msg: Message) !void {
        try self.buffer.append(msg);

        const now = std.time.nanoTimestamp();

        // è¾¾åˆ°æ‰¹é‡å¤§å°æˆ–è¶…æ—¶æ—¶åˆ·æ–°
        if (self.buffer.items.len >= self.flush_size or
            now - self.last_flush >= self.flush_interval)
        {
            try self.flush();
        }
    }

    fn flush(self: *BatchProcessor) !void {
        if (self.buffer.items.len == 0) return;

        // æ‰¹é‡å‘é€
        try sendBatch(self.buffer.items);

        self.buffer.clearRetainingCapacity();
        self.last_flush = std.time.nanoTimestamp();
    }
};
```

### 5.2 é›¶æ‹·è´

```zig
// src/network/zero_copy.zig

pub const ZeroCopyBuffer = struct {
    mmap_region: []align(std.mem.page_size) u8,
    read_offset: usize,
    write_offset: usize,

    pub fn init(size: usize) !ZeroCopyBuffer {
        const mmap_region = try std.os.mmap(
            null,
            size,
            std.os.PROT.READ | std.os.PROT.WRITE,
            std.os.MAP.PRIVATE | std.os.MAP.ANONYMOUS,
            -1,
            0,
        );

        return .{
            .mmap_region = mmap_region,
            .read_offset = 0,
            .write_offset = 0,
        };
    }

    pub fn deinit(self: *ZeroCopyBuffer) void {
        std.os.munmap(self.mmap_region);
    }

    pub fn getWriteSlice(self: *ZeroCopyBuffer) []u8 {
        return self.mmap_region[self.write_offset..];
    }

    pub fn advance(self: *ZeroCopyBuffer, bytes: usize) void {
        self.write_offset += bytes;
    }
};
```

---

## 6. æ€§èƒ½æµ‹é‡

### 6.1 Profiling å·¥å…·

```zig
// src/performance/profiler.zig

pub const Profiler = struct {
    samples: std.StringHashMap(Sample),
    allocator: std.mem.Allocator,

    pub const Sample = struct {
        count: u64,
        total_ns: u64,
        min_ns: u64,
        max_ns: u64,

        pub fn record(self: *Sample, duration_ns: u64) void {
            self.count += 1;
            self.total_ns += duration_ns;
            self.min_ns = @min(self.min_ns, duration_ns);
            self.max_ns = @max(self.max_ns, duration_ns);
        }

        pub fn avg(self: Sample) u64 {
            if (self.count == 0) return 0;
            return self.total_ns / self.count;
        }
    };

    pub fn init(allocator: std.mem.Allocator) Profiler {
        return .{
            .samples = std.StringHashMap(Sample).init(allocator),
            .allocator = allocator,
        };
    }

    pub fn start(self: *Profiler, name: []const u8) i64 {
        return std.time.nanoTimestamp();
    }

    pub fn end(self: *Profiler, name: []const u8, start_time: i64) void {
        const duration = @as(u64, @intCast(std.time.nanoTimestamp() - start_time));

        const entry = self.samples.getPtr(name) orelse blk: {
            self.samples.put(name, Sample{
                .count = 0,
                .total_ns = 0,
                .min_ns = std.math.maxInt(u64),
                .max_ns = 0,
            }) catch return;
            break :blk self.samples.getPtr(name).?;
        };

        entry.record(duration);
    }

    pub fn report(self: *Profiler) void {
        std.debug.print("\nPerformance Report:\n", .{});
        std.debug.print("{s:<30} {s:>10} {s:>10} {s:>10} {s:>10}\n", .{
            "Operation",
            "Count",
            "Avg (Âµs)",
            "Min (Âµs)",
            "Max (Âµs)",
        });
        std.debug.print("{s}\n", .{"-" ** 80});

        var iter = self.samples.iterator();
        while (iter.next()) |entry| {
            const sample = entry.value_ptr.*;
            std.debug.print("{s:<30} {d:>10} {d:>10} {d:>10} {d:>10}\n", .{
                entry.key_ptr.*,
                sample.count,
                sample.avg() / 1000,
                sample.min_ns / 1000,
                sample.max_ns / 1000,
            });
        }
    }
};

// ä½¿ç”¨ç¤ºä¾‹
var profiler = Profiler.init(allocator);

const start = profiler.start("order_submit");
try submitOrder(request);
profiler.end("order_submit", start);

// ç¨‹åºç»“æŸæ—¶
profiler.report();
```

### 6.2 å»¶è¿Ÿç›´æ–¹å›¾

```zig
// src/performance/latency_histogram.zig

pub const LatencyHistogram = struct {
    buckets: [20]u64,  // å¯¹æ•°åˆ»åº¦æ¡¶
    count: u64,
    sum: u64,

    const BUCKET_BOUNDS = [_]u64{
        1_000,      // 1Âµs
        2_000,      // 2Âµs
        5_000,      // 5Âµs
        10_000,     // 10Âµs
        20_000,     // 20Âµs
        50_000,     // 50Âµs
        100_000,    // 100Âµs
        200_000,    // 200Âµs
        500_000,    // 500Âµs
        1_000_000,  // 1ms
        2_000_000,  // 2ms
        5_000_000,  // 5ms
        10_000_000, // 10ms
        // ...
    };

    pub fn init() LatencyHistogram {
        return .{
            .buckets = [_]u64{0} ** 20,
            .count = 0,
            .sum = 0,
        };
    }

    pub fn record(self: *LatencyHistogram, latency_ns: u64) void {
        self.count += 1;
        self.sum += latency_ns;

        for (BUCKET_BOUNDS, 0..) |bound, i| {
            if (latency_ns < bound) {
                self.buckets[i] += 1;
                return;
            }
        }
        self.buckets[self.buckets.len - 1] += 1;
    }

    pub fn percentile(self: *LatencyHistogram, p: f64) u64 {
        const target = @as(u64, @intFromFloat(@as(f64, @floatFromInt(self.count)) * p));
        var cumulative: u64 = 0;

        for (BUCKET_BOUNDS, 0..) |bound, i| {
            cumulative += self.buckets[i];
            if (cumulative >= target) {
                return bound;
            }
        }

        return BUCKET_BOUNDS[BUCKET_BOUNDS.len - 1];
    }

    pub fn report(self: *LatencyHistogram) void {
        std.debug.print("Latency Distribution:\n", .{});
        std.debug.print("  Count: {d}\n", .{self.count});
        std.debug.print("  Mean: {d} Âµs\n", .{self.sum / self.count / 1000});
        std.debug.print("  P50: {d} Âµs\n", .{self.percentile(0.50) / 1000});
        std.debug.print("  P95: {d} Âµs\n", .{self.percentile(0.95) / 1000});
        std.debug.print("  P99: {d} Âµs\n", .{self.percentile(0.99) / 1000});
        std.debug.print("  P99.9: {d} Âµs\n", .{self.percentile(0.999) / 1000});
    }
};
```

---

## 7. æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

### ç¼–è¯‘ä¼˜åŒ–
- [ ] ä½¿ç”¨ ReleaseFast æˆ– ReleaseSafe
- [ ] å¯ç”¨ LTO
- [ ] é’ˆå¯¹ç›®æ ‡ CPU ä¼˜åŒ–
- [ ] ç§»é™¤æœªä½¿ç”¨ä»£ç 

### å†…å­˜ä¼˜åŒ–
- [ ] ä½¿ç”¨ Arena Allocator å¤„ç†ä¸´æ—¶æ•°æ®
- [ ] å®ç°å¯¹è±¡æ± å‡å°‘åˆ†é…
- [ ] é¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´
- [ ] ç›‘æ§å†…å­˜æ³„æ¼

### æ•°æ®ç»“æ„
- [ ] é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„
- [ ] ä¿æŒç¼“å­˜å‹å¥½çš„å†…å­˜å¸ƒå±€
- [ ] ä½¿ç”¨å›ºå®šå¤§å°æ•°ç»„æ›¿ä»£åŠ¨æ€åˆ†é…
- [ ] é¢„åˆ†é…å®¹é‡é¿å…åŠ¨æ€æ‰©å®¹

### å¹¶å‘
- [ ] ä½¿ç”¨æ— é”æ•°æ®ç»“æ„
- [ ] é¿å…é”ç«äº‰
- [ ] åˆç†ä½¿ç”¨çº¿ç¨‹æ± 
- [ ] å‡å°‘åŒæ­¥å¼€é”€

### I/O
- [ ] æ‰¹é‡å¤„ç†å‡å°‘ç³»ç»Ÿè°ƒç”¨
- [ ] ä½¿ç”¨å¼‚æ­¥ I/O
- [ ] å®ç°é›¶æ‹·è´ä¼ è¾“
- [ ] å¤ç”¨è¿æ¥

### æµ‹é‡
- [ ] æŒç»­æµ‹é‡å…³é”®è·¯å¾„å»¶è¿Ÿ
- [ ] è®°å½•æ€§èƒ½åŸºçº¿
- [ ] ç›‘æ§æ€§èƒ½é€€åŒ–
- [ ] å®šæœŸè¿›è¡Œæ€§èƒ½æµ‹è¯•

---

*Last updated: 2025-01*
